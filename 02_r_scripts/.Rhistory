# import the data
load("~/Desktop/PhD_Learning/HUDM6026 Computational Statistics/HUDM6026_Final_Project/01_data/02_processed_data/hsls_sub.rdata")
names(hsls_sub)
model <- lm(X3TGPASTEM~., data = hsls_sub)
summary(model)
## Global options
knitr::opts_chunk$set(
cache = TRUE,
prompt = TRUE,
comment = '',
collapse = TRUE,
message = FALSE)
load("~/Desktop/PhD_Learning/HUDM6026 Computational Statistics/HUDM6026_Final_Project/01_data/02_processed_data/hsls_sub.rdata")
(n = dim(hsls_sub)[1])
(mu_stem = mean(hsls_sub$X3TGPASTEM))
(mu_scor = mean(hsls_sub$X1TXMTSCOR))
(sd_stem = sd(hsls_sub$X3TGPASTEM))
(sd_scor = sd(hsls_sub$X1TXMTSCOR))
table(hsls_sub$X3TGPASTEM)
cor(hsls_sub$X3TGPASTEM,hsls_sub$X1TXMTSCOR)
# plot(hsls_sub$X3TGPASTEM,hsls_sub$X1TXMTSCOR)
# plot(hsls_sub$X1TXMTSCOR,hsls_sub$X3TGPASTEM)
model_lm <- lm(X3TGPASTEM ~ X1TXMTSCOR, data = hsls_sub)
summary(model_lm)
dat_gen <- function(size= 500,  # smaple size
betas,      # a numeric array of betas
iv_mean,    # predictor's mean
iv_var,     # predictor's variance
error_sd){  # residuals' sd
# data mainly are generated from a normal distribution ~ N(iv_mean, iv_sd)
X <- rnorm(size, mean = iv_mean, sd= sqrt(iv_var))
X_aug <- cbind(1, X)
# residuals are generated from ~N(0, sd)
Error <- rnorm(size, mean=0, sd=error_sd)
# based on the parameters to generate the outcomes
Y <- X_aug %*% as.matrix(betas) + Error
out <- cbind(Y, X)
colnames(out) <- c("Y", "X1")
return(as.data.frame(out))
}
reg <- function(ds) {
x <- as.matrix(ds[,2])
y <- as.matrix(ds[,1])
y_cen <- apply(y, 2, function(x) x-mean(x))
x_cen <- apply(x, 2, function(x) x-mean(x))
# the OLS method
b1 <- sum(x_cen*y_cen)/sum(x_cen^2)
b0 <- mean(y - x*b1)
y_hat <- b0 + x*b1
sse <- sum((y-y_hat)^2)
sig_sq <- sse/(nrow(x)-2)
# the alternative method
b1_a <- sum(y_cen/x_cen)/nrow(x)
b0_a <- mean(y - x*b1_a)
y_hat_a <- b0_a + x*b1_a
sse_a <- sum((y-y_hat_a)^2)
sig_sq_a <- sse_a/nrow(x)
out_ <- cbind(b0, b1, sig_sq, b0_a, b1_a,sig_sq_a)
return(out_)
}
R <- 1000
set.seed(666)
# randomly generate 1000 samples
dat_list <- replicate(n = R,
expr = dat_gen(size = 40,
betas = c(-0.265,0.053),
iv_mean = 51.24985, iv_var = 100.6209,
error_sd = 0.7693),
simplify = FALSE)
# estimated the simple regression model on each sample
estimates <- sapply(X = dat_list,
FUN = reg,
simplify = TRUE)
estimates <- t(estimates)
colnames(estimates) <- c("b0", "b1", "sig_sq", "b0_a", "b1_a", "sig_sq_a")
(estimates_hat_mean <- round(apply(estimates,2,mean),3))
(estimates_hat_sd <- round(apply(estimates,2,sd),3))
(estimates_hat_sd <- round(apply(estimates,2,var),3))
59.657^2
0.645^2
(estimates_hat_se <- round(apply(estimates,2,function(x) sd(x)/sqrt(R)),3))
935.836/ sqrt(100)
935.836/ sqrt(1000)
length(estimates)
a <- matrix(c(1,2,3,4),4,4)
a
b <- matrix(1,4,4)
a-b
b
b <- matrix(c(1,2,3,4),4,4,byrow = F)
b
a
b <- matrix(c(1,2,3,4),4,4,byrow = T)
b
a-b
(a-b)^2
theta <- matrix(c(-0.265, 0.053,0.7693), 6, nrow(estimates), byrow = T)
head(theta)
dim(theta)
theta <- matrix(c(-0.265, 0.053,0.7693), 6, nrow(estimates), byrow = F)
dim(theta)
theta <- matrix(c(-0.265, 0.053,0.7693), nrow(estimates),6 , byrow = T)
dim(theta)
head(theta)
head(estimates)
estimates-theta_m
theta_m <- matrix(c(-0.265, 0.053,0.7693), nrow(estimates),6 , byrow = T)
estimates-theta_m
dim(estimates)
a <- matrix(c(1,2,3,4),4,4)
a
b <- matrix(c(1,2,3,4),4,4,byrow = T)
(a-b)^2
a <- matrix(c(1,2,3,4),4,4)
a
b <- matrix(c(1,2,3,4),4,4,byrow = T)
ab <- a-b
ab[,1]^2
sum(ab[,1]^2)
ab^2
R <- 1000
set.seed(666)
# randomly generate 1000 samples
dat_list <- replicate(n = R,
expr = dat_gen(size = 40,
betas = c(-0.265,0.053),
iv_mean = 51.24985, iv_var = 100.6209,
error_sd = 0.7693),
simplify = FALSE)
# estimated the simple regression model on each sample
estimates <- sapply(X = dat_list,
FUN = reg,
simplify = TRUE)
estimates <- t(estimates)
colnames(estimates) <- c("b0", "b1", "sig_sq", "b0_a", "b1_a", "sig_sq_a")
(estimates_hat_mean <- round(apply(estimates,2,mean),3))
(estimates_hat_var <- round(apply(estimates,2,var),3))
(estimates_hat_se <- round(apply(estimates,2,function(x) sd(x)/sqrt(R)),3))
# to calculate the MSE
# first to make a parameter matrix in shape of the estimates matrix
theta_m <- matrix(c(-0.265, 0.053,0.7693), nrow(estimates),6 , byrow = T)
# use the estimates matrix minus the parameter matrix
est_cent <-estimates-theta_m
# use apply to get the mse for each estimates
(estimates_mse <- round(apply(est_cent,2,function(x) sum(x^2)/R,3)))
121.135**2
121.135**2+875788.198
0.093**2 + 1.317
0.007*0.007 + 0.017
par_m <- matrix(c(-0.265, 0.053,0.7693), 6,1)
par_m
R <- 1000
set.seed(666)
# randomly generate 1000 samples
dat_list <- replicate(n = R,
expr = dat_gen(size = 40,
betas = c(-0.265,0.053),
iv_mean = 51.24985, iv_var = 100.6209,
error_sd = 0.7693),
simplify = FALSE)
# estimated the simple regression model on each sample
estimates <- sapply(X = dat_list,
FUN = reg,
simplify = TRUE)
estimates <- t(estimates)
colnames(estimates) <- c("b0", "b1", "sig_sq", "b0_a", "b1_a", "sig_sq_a")
# to calculate the MSE
# first to make a parameter matrix in shape of the estimates matrix
theta_m <- matrix(c(-0.265, 0.053,0.7693), nrow(estimates),6 , byrow = T)
# use the estimates matrix minus the parameter matrix
est_cent <-estimates-theta_m
# use apply to get the mse for each estimates
estimates_hat_mean <- round(apply(estimates,2,mean),3)
estimates_hat_var <- round(apply(estimates,2,var),3)
estimates_hat_se <- round(apply(estimates,2,function(x) sd(x)/sqrt(R)),3)
estimates_mse <- round(apply(est_cent,2,function(x) sum(x^2)/R),3)
results <- rbind(estimates_hat_mean,estimates_hat_var,
estimates_hat_se,estimates_mse)
rownames(results) <- c("Mean", "Variance","SE", "MSE")
results_trans <- t(results)
# add a column to calculate the bias
par_m <- matrix(c(-0.265, 0.053,0.7693), 6,1)
results_trans$Bias <- results_trans$Mean - par_m
results_trans
results_trans$Mean
data_b
## Global options
knitr::opts_chunk$set(
cache = TRUE,
prompt = TRUE,
comment = '',
collapse = TRUE,
message = FALSE)
load("~/Desktop/PhD_Learning/HUDM6026 Computational Statistics/HUDM6026_Final_Project/01_data/02_processed_data/hsls_sub.rdata")
(n = dim(hsls_sub)[1])
(mu_stem = mean(hsls_sub$X3TGPASTEM))
(mu_scor = mean(hsls_sub$X1TXMTSCOR))
(sd_stem = sd(hsls_sub$X3TGPASTEM))
(sd_scor = sd(hsls_sub$X1TXMTSCOR))
table(hsls_sub$X3TGPASTEM)
cor(hsls_sub$X3TGPASTEM,hsls_sub$X1TXMTSCOR)
# plot(hsls_sub$X3TGPASTEM,hsls_sub$X1TXMTSCOR)
# plot(hsls_sub$X1TXMTSCOR,hsls_sub$X3TGPASTEM)
model_lm <- lm(X3TGPASTEM ~ X1TXMTSCOR, data = hsls_sub)
summary(model_lm)
dat_gen <- function(size= 500,  # smaple size
betas,      # a numeric array of betas
iv_mean,    # predictor's mean
iv_var,     # predictor's variance
error_sd){  # residuals' sd
# data mainly are generated from a normal distribution ~ N(iv_mean, iv_sd)
X <- rnorm(size, mean = iv_mean, sd= sqrt(iv_var))
X_aug <- cbind(1, X)
# residuals are generated from ~N(0, sd)
Error <- rnorm(size, mean=0, sd=error_sd)
# based on the parameters to generate the outcomes
Y <- X_aug %*% as.matrix(betas) + Error
out <- cbind(Y, X)
colnames(out) <- c("Y", "X1")
return(as.data.frame(out))
}
reg <- function(ds) {
x <- as.matrix(ds[,2])
y <- as.matrix(ds[,1])
y_cen <- apply(y, 2, function(x) x-mean(x))
x_cen <- apply(x, 2, function(x) x-mean(x))
# the OLS method
b1 <- sum(x_cen*y_cen)/sum(x_cen^2)
b0 <- mean(y - x*b1)
y_hat <- b0 + x*b1
sse <- sum((y-y_hat)^2)
sig_sq <- sse/(nrow(x)-2)
# the alternative method
b1_a <- sum(y_cen/x_cen)/nrow(x)
b0_a <- mean(y - x*b1_a)
y_hat_a <- b0_a + x*b1_a
sse_a <- sum((y-y_hat_a)^2)
sig_sq_a <- sse_a/nrow(x)
out_ <- cbind(b0, b1, sig_sq, b0_a, b1_a,sig_sq_a)
return(out_)
}
R <- 1000
set.seed(666)
# randomly generate 1000 samples
dat_list <- replicate(n = R,
expr = dat_gen(size = 40,
betas = c(-0.265,0.053),
iv_mean = 51.24985, iv_var = 100.6209,
error_sd = 0.7693),
simplify = FALSE)
# estimated the simple regression model on each sample
estimates <- sapply(X = dat_list,
FUN = reg,
simplify = TRUE)
estimates <- t(estimates)
colnames(estimates) <- c("b0", "b1", "sig_sq", "b0_a", "b1_a", "sig_sq_a")
# write a function to calculate the estimates
est_out <- function(esti_mat, size){
# to calculate the MSE
# first to make a parameter matrix in shape of the estimates matrix
theta_m <- matrix(c(-0.265, 0.053,0.7693), nrow(esti_mat),6 , byrow = T)
# use the estimates matrix minus the parameter matrix
est_cent <-esti_mat-theta_m
# use apply to get the mse for each estimates
estimates_hat_mean <- round(apply(esti_mat,2,mean),3)
estimates_hat_var <- round(apply(esti_mat,2,var),3)
estimates_hat_se <- round(apply(esti_mat,2,function(x) sd(x)/sqrt(size)),3)
estimates_mse <- round(apply(est_cent,2,function(x) sum(x^2)/size),3)
results <- rbind(estimates_hat_mean,estimates_hat_var,
estimates_hat_se,estimates_mse)
rownames(results) <- c("Mean", "Variance","SE", "MSE")
results_trans <- as.data.frame(t(results))
# add a column to calculate the bias
par_m <- matrix(c(-0.265, 0.053,0.7693), 6,1)
results_trans$Bias <- results_trans$Mean - par_m
results_trans$Parameter <- par_m
results_trans <- results_trans[,c(6,1,3,2,5,4)]
return(results_trans)
}
est_out(estimates, R)
# generate a single dataset
data_b <-dat_gen(size = 40,betas = c(-0.265,0.053),
iv_mean = 51.24985, iv_var = 100.6209,
error_sd = 0.7693)
# run bootstrapping on this single dataset
B = 1000
# shuffle the 1:40 index rather than data_b
boot_index <- replicate(n=B,
expr = sample(1:40, 40, TRUE),
simplify = FALSE)
# use the bootstrapped indices to extracted the data
boot_samp <- list()
for (i in 1:1000) {
boot_unit <- data_b[boot_index[[i]],]
boot_samp[[i]] <- boot_unit
}
estimates <- sapply(X = boot_samp,
FUN = reg,
simplify = TRUE)
estimates <- t(estimates)
colnames(estimates) <- c("b0", "b1", "sig_sq", "b0_a", "b1_a", "sig_sq_a")
est_out(estimates, B)
jack_list <- list()
for (i in 1:40) {
data_loov <- data_b[-i,]
jack_list[[i]] <- data_loov
}
estimates <- sapply(X = jack_list,
FUN = reg,
simplify = TRUE)
estimates <- t(estimates)
colnames(estimates) <- c("b0", "b1", "sig_sq", "b0_a", "b1_a", "sig_sq_a")
data_b
# estimate the bias
# first to make a parameter matrix in shape of the estimates matrix
theta_m <- matrix(c(-0.265, 0.053,0.7693), nrow(esti_mat),6 , byrow = T)
data_b
data_b_mean <- apply(data_b, 2, mean)
data_b_mean
-0.2593^2
-0.2680^2
-0.2680^2 +0.01
(-0.2680)^2 +0.01
## Global options
knitr::opts_chunk$set(
cache = TRUE,
prompt = TRUE,
comment = '',
collapse = TRUE,
message = FALSE)
load("~/Desktop/PhD_Learning/HUDM6026 Computational Statistics/HUDM6026_Final_Project/01_data/02_processed_data/hsls_sub.rdata")
(n = dim(hsls_sub)[1])
(mu_stem = mean(hsls_sub$X3TGPASTEM))
(mu_scor = mean(hsls_sub$X1TXMTSCOR))
(sd_stem = sd(hsls_sub$X3TGPASTEM))
(sd_scor = sd(hsls_sub$X1TXMTSCOR))
table(hsls_sub$X3TGPASTEM)
cor(hsls_sub$X3TGPASTEM,hsls_sub$X1TXMTSCOR)
# plot(hsls_sub$X3TGPASTEM,hsls_sub$X1TXMTSCOR)
# plot(hsls_sub$X1TXMTSCOR,hsls_sub$X3TGPASTEM)
model_lm <- lm(X3TGPASTEM ~ X1TXMTSCOR, data = hsls_sub)
summary(model_lm)
dat_gen <- function(size= 500,  # smaple size
betas,      # a numeric array of betas
iv_mean,    # predictor's mean
iv_var,     # predictor's variance
error_sd){  # residuals' sd
# data mainly are generated from a normal distribution ~ N(iv_mean, iv_sd)
X <- rnorm(size, mean = iv_mean, sd= sqrt(iv_var))
X_aug <- cbind(1, X)
# residuals are generated from ~N(0, sd)
Error <- rnorm(size, mean=0, sd=error_sd)
# based on the parameters to generate the outcomes
Y <- X_aug %*% as.matrix(betas) + Error
out <- cbind(Y, X)
colnames(out) <- c("Y", "X1")
return(as.data.frame(out))
}
reg <- function(ds) {
x <- as.matrix(ds[,2])
y <- as.matrix(ds[,1])
y_cen <- apply(y, 2, function(x) x-mean(x))
x_cen <- apply(x, 2, function(x) x-mean(x))
# the OLS method
b1 <- sum(x_cen*y_cen)/sum(x_cen^2)
b0 <- mean(y - x*b1)
y_hat <- b0 + x*b1
sse <- sum((y-y_hat)^2)
sig_sq <- sse/(nrow(x)-2)
# the alternative method
b1_a <- sum(y_cen/x_cen)/nrow(x)
b0_a <- mean(y - x*b1_a)
y_hat_a <- b0_a + x*b1_a
sse_a <- sum((y-y_hat_a)^2)
sig_sq_a <- sse_a/nrow(x)
out_ <- cbind(b0, b1, sig_sq, b0_a, b1_a,sig_sq_a)
return(out_)
}
R <- 1000
set.seed(666)
# randomly generate 1000 samples
dat_list <- replicate(n = R,
expr = dat_gen(size = 40,
betas = c(-0.265,0.053),
iv_mean = 51.24985, iv_var = 100.6209,
error_sd = 0.7693),
simplify = FALSE)
# estimated the simple regression model on each sample
estimates <- sapply(X = dat_list,
FUN = reg,
simplify = TRUE)
estimates <- t(estimates)
colnames(estimates) <- c("b0", "b1", "sig_sq", "b0_a", "b1_a", "sig_sq_a")
# write a function to calculate the estimates
est_out <- function(esti_mat, size){
# to calculate the MSE
# first to make a parameter matrix in shape of the estimates matrix
theta_m <- matrix(c(-0.265, 0.053,0.7693), nrow(esti_mat),6 , byrow = T)
# use the estimates matrix minus the parameter matrix
est_cent <-esti_mat-theta_m
# use apply to get the mse for each estimates
estimates_hat_mean <- round(apply(esti_mat,2,mean),3)
estimates_hat_var <- round(apply(esti_mat,2,var),3)
estimates_hat_se <- round(apply(esti_mat,2,function(x) sd(x)/sqrt(size)),3)
estimates_mse <- round(apply(est_cent,2,function(x) sum(x^2)/size),3)
results <- rbind(estimates_hat_mean,estimates_hat_var,
estimates_hat_se,estimates_mse)
rownames(results) <- c("Mean", "Variance","SE", "MSE")
results_trans <- as.data.frame(t(results))
# add a column to calculate the bias
par_m <- matrix(c(-0.265, 0.053,0.7693), 6,1)
results_trans$Bias <- results_trans$Mean - par_m
results_trans$Parameter <- par_m
results_trans <- results_trans[,c(6,1,3,2,5,4)]
return(results_trans)
}
(mc_out <- est_out(estimates, R))
# generate a single dataset
data_b <-dat_gen(size = 40,betas = c(-0.265,0.053),
iv_mean = 51.24985, iv_var = 100.6209,
error_sd = 0.7693)
# run bootstrapping on this single dataset
B = 1000
# shuffle the 1:40 index rather than data_b
boot_index <- replicate(n=B,
expr = sample(1:40, 40, TRUE),
simplify = FALSE)
# use the bootstrapped indices to extracted the data
boot_samp <- list()
for (i in 1:1000) {
boot_unit <- data_b[boot_index[[i]],]
boot_samp[[i]] <- boot_unit
}
estimates <- sapply(X = boot_samp,
FUN = reg,
simplify = TRUE)
estimates <- t(estimates)
colnames(estimates) <- c("b0", "b1", "sig_sq", "b0_a", "b1_a", "sig_sq_a")
(bs_out<-est_out(estimates, B))
jack_list <- list()
for (i in 1:40) {
# each round drop the ith observation
data_loov <- data_b[-i,]
# make a list to load each jacknife sample
jack_list[[i]] <- data_loov
}
estimates <- sapply(X = jack_list,
FUN = reg,
simplify = TRUE)
estimates <- t(estimates)
# this is a 40 x 6 matrix
colnames(estimates) <- c("b0", "b1", "sig_sq", "b0_a", "b1_a", "sig_sq_a")
# since the population parameter is known, we use them directly
(jn_out<-est_out(estimates, 40))
# this part won't display in the final rmd pdf, but it still runs to output the table
final_result <- rbind(mc_out, bs_out, jn_out)
class(final_result)
write.csv(final_result, "..\03_outputs\02_final_results.csv")
# this part won't display in the final rmd pdf, but it still runs to output the table
final_result <- rbind(mc_out, bs_out, jn_out)
write.csv(final_result, "..\03_outputs\02_final_results.csv")
write.csv(final_result, "...\03_outputs\02_final_results.csv")
write.csv(final_result, ".\03_outputs\02_final_results.csv")
final_result <- rbind(mc_out, bs_out, jn_out)
write.csv(final_result, "./03_outputs/02_final_results.csv")
write.csv(final_result, "~/03_outputs/02_final_results.csv")
write.csv(final_result, "~/Desktop/PhD_Learning/HUDM6026 Computational Statistics/HUDM6026_Final_Project/03_outputs/02_final_results.csv")
install.packages("distill")
install.packages("radix")
install.packages("rticles")
install.packages('tinytex')
tinytex::install_tinytex()
install.packages("tinytex")
tinytex::install_tinytex()
